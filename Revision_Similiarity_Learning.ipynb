{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Similiarity_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmuQZEVokSO7",
        "colab_type": "code",
        "outputId": "55af0185-7faf-462d-a80f-93b5453c0b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwRMb8gCjJNx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "72d88b10-c0ad-4a2e-fbbe-3f8668b779b9"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras \n",
        "import cv2\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ilO22HgjoFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(2) # Python\n",
        "np.random.seed(1) #numpy\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(3) # Tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-iVE9CSjsQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The path to the omniglot data\n",
        "root_path = \"./drive/My Drive/\"\n",
        "train_path = os.path.join(root_path,'training') \n",
        "validation_path = os.path.join(root_path,'validation')\n",
        "\n",
        "def load_image(path, n = 0):\n",
        "    X = []\n",
        "    \n",
        "    #Load every srud seperately and place that in one tensor\n",
        "    for student in os.listdir(path):\n",
        "#         print(\"Loading student: \" + student)\n",
        "        student_path = os.path.join(path, student)\n",
        "       \n",
        "        category_images = []\n",
        "            \n",
        "        if not os.path.isdir(student_path):\n",
        "                continue\n",
        "            \n",
        "            #Read evey image with in the directory\n",
        "        for filename in os.listdir(student_path):\n",
        "                image_path = os.path.join(student_path, filename)\n",
        "                image = imageio.imread(image_path)\n",
        "                width = 105\n",
        "                height = 105 # keep original height\n",
        "                dim = (height,width)\n",
        " \n",
        "                # resize image\n",
        "                image = cv2.resize(image, dim)\n",
        "                image=cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "                image= np.expand_dims(image, axis=0)\n",
        "#                 print(image_path)\n",
        "                #Image preprocessing\n",
        "                image = image/255\n",
        "                image = 1 - image\n",
        "                \n",
        "                X.append(image)\n",
        "                \n",
        "        \n",
        "    X = np.stack(X)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1XPY1NDwZQ4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiVe4riutJia",
        "colab_type": "code",
        "outputId": "65101eb8-f2c1-48fe-8227-7af0b749c385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Loading Training Set\")\n",
        "Xtrain = load_image(train_path)\n",
        "print(Xtrain.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Training Set\n",
            "(1470, 1, 105, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDx2NkDEtbtp",
        "colab_type": "code",
        "outputId": "64c67ad7-3c85-4c1f-b9f8-b20c44bbe8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Now loading evaluation set\")\n",
        "Xval = load_image(validation_path)\n",
        "print(Xval.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now loading evaluation set\n",
            "(646, 1, 105, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA7bnLjpkMX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(data, batch_size):\n",
        "    n_classes, n_examples, h, w = data.shape\n",
        "    \n",
        "    pairs = [np.zeros((batch_size, 1, h, w)) for i in range(2)]\n",
        "    \n",
        "    targets = np.zeros((batch_size,))\n",
        "    targets[batch_size//2:] = 1\n",
        "    \n",
        "    categories = np.random.choice(n_classes, size = (batch_size), replace = False)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        category = categories[i]\n",
        "        \n",
        "        idx1 = np.random.randint(0, n_examples)\n",
        "        pairs[0][i,:,:,:] = data[category, idx1].reshape(1, h,w)\n",
        "        idx2 = np.random.randint(0, n_examples)\n",
        "        \n",
        "        if targets[i] == 0:\n",
        "            category_2 = category\n",
        "        else:\n",
        "            category_2 = (category + np.random.randint(1, n_classes)) % n_classes\n",
        " \n",
        "        \n",
        "        pairs[1][i,:,:,:] = data[category_2, idx2].reshape(1, h, w)\n",
        "        \n",
        "    return pairs, targets\n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gZYIb6Opj6vu",
        "colab": {}
      },
      "source": [
        "def generate(data,batch_size):\n",
        "        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
        "        while True:\n",
        "            pairs, targets = get_batch(Xtrain,batch_size)\n",
        "            yield (pairs, targets)    \n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQo6r5PdW9bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_oneshot_task(N,data,language=None):\n",
        "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
        "        n_classes, n_examples, w, h = data.shape\n",
        "        indices = np.random.randint(0,n_examples,size=(N,))\n",
        "        categories = np.random.choice(range(n_classes),size=(N,),replace=False)            \n",
        "        true_category = categories[0]\n",
        "        ex1 = np.random.randint(0, n_examples)\n",
        "        ex2 = np.random.randint(0, n_examples)\n",
        "        # ex1, ex2 = np.random.choice(n_examples,replace=False,size=(0,))\n",
        "        test_image = np.asarray([data[true_category,ex1,:,:]]*N).reshape(N, 1, w,h)\n",
        "        support_set = data[categories,indices,:,:]\n",
        "        support_set[0,:,:] = data[true_category,ex2]\n",
        "        support_set = support_set.reshape(N, 1, w,h)\n",
        "        targets = np.zeros((N,))\n",
        "        targets[0] = 1\n",
        "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
        "        pairs = [test_image,support_set]\n",
        "\n",
        "        return pairs, targets\n",
        "    \n",
        "\n",
        "                   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BveJBYSGXBSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_oneshot(model,N,k,data,verbose=0):\n",
        "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
        "        n_correct = 0\n",
        "        if verbose:\n",
        "            print(\"Evaluating model on {} random {} way one-shot learning tasks ...\".format(k,N))\n",
        "        for i in range(k):\n",
        "            inputs, targets = make_oneshot_task(N,data)\n",
        "            probs = model.predict(inputs)\n",
        "            if np.argmax(probs) == np.argmax(targets):\n",
        "                n_correct+=1\n",
        "        percent_correct = (100.0*n_correct / k)\n",
        "        if verbose:\n",
        "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
        "        return percent_correct\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G7OVaeFXD8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, epochs, verbosity):\n",
        "        model.fit_generator(self.generate(batch_size),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHye1P61Kwgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input, Lambda\n",
        "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB_F4h-QK4k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Contrastive Loss\n",
        "def euclid_dist(input_pair):\n",
        "    x, y = input_pair\n",
        "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
        "    return distance\n",
        "\n",
        "def euclid_dist_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0],1)\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    y_true = -1 * y_true + 1\n",
        "    return K.mean((1-y_true) * K.square(y_pred) + y_true *  K.square(K.maximum(margin - y_pred, 0.0)))\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "    ones = K.ones_like(y_pred)\n",
        "    return K.mean(K.equal(y_true, ones - K.clip(K.round(y_pred), 0, 1)), axis=-1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLQRUIh2CDMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INIT_WEIGHTS = os.path.join(root_path, 'init_weights.hdf5')\n",
        "CHECKPOINTED_WEIGHTS = os.path.join(root_path, 'checkpointed_weights.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAf2dXI0STPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUkf3OQeLdLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_width  =105\n",
        "im_height =105\n",
        "im_chan   =1      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dc2DWlYoNDVV",
        "outputId": "73cf059a-13cb-451f-a123-caf490c1e24e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "input_img =Input(( im_chan,im_height,im_width ), name='img')\n",
        "\n",
        "# print(input_img)\n",
        "# Down Block 1\n",
        "c1 = Conv2D(32,(1,1), activation='relu', kernel_regularizer=l2(2e-4) )(input_img)\n",
        "p1 = MaxPooling2D(data_format=\"channels_first\") (c1)\n",
        "\n",
        "#Down Block 2\n",
        "c2 = Conv2D(128, (1, 1), activation='relu', kernel_regularizer=l2(2e-4)) (p1)\n",
        "p2 = MaxPooling2D(data_format=\"channels_first\") (c2)\n",
        "\n",
        "#Down Block 3\n",
        "c3 = Conv2D(128, (1, 1), activation='relu', kernel_regularizer=l2(2e-4)) (p2)\n",
        "p3 = MaxPooling2D(data_format=\"channels_first\") (c3)\n",
        "\n",
        "#Down Block 4\n",
        "c4 = Conv2D(256, (1, 1), activation='relu', kernel_regularizer=l2(2e-4)) (p3)\n",
        "p4 = MaxPooling2D(data_format=\"channels_first\") (c4)\n",
        "# # print(p4)\n",
        "flat = Flatten()(p4)\n",
        "output = Dense(1, activation='sigmoid')(flat)\n",
        "\n",
        "# Instantiate the Model\n",
        "model = Model(input_img, output)\n",
        "\n",
        "input1 = Input(shape = (1,105,105))\n",
        "input2 = Input(shape = (1,105,105))\n",
        "\n",
        "output1 = model(input1)\n",
        "output2 = model(input2)\n",
        "\n",
        "\n",
        "distance = Lambda(euclid_dist, output_shape=euclid_dist_shape)([output1, output2])\n",
        "\n",
        "prediction = Dense(1, activation='sigmoid')(distance)\n",
        "    \n",
        "siamese = Model(inputs=[input1, input2], outputs=prediction)\n",
        "    \n",
        "optimizer = Adam(lr = 0.00006)\n",
        "    \n",
        "siamese.compile(loss=contrastive_loss, optimizer=optimizer, metrics=[acc])\n",
        "\n",
        "siamese.summary()\n",
        "os.system(\"rm {}\".format(INIT_WEIGHTS))\n",
        "siamese.save_weights(INIT_WEIGHTS)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 105, 105)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 1, 105, 105)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Model)                 (None, 1)            31297       input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1)            0           model_1[1][0]                    \n",
            "                                                                 model_1[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            2           lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 31,299\n",
            "Trainable params: 31,299\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCODHWVjRJs4",
        "colab_type": "code",
        "outputId": "4fd7b976-55e6-4f0e-9da9-9cd8fa3bff8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Training loop\n",
        "print(\"!\")\n",
        "evaluate_every = 1 # interval for evaluating on one-shot tasks\n",
        "loss_every=50 # interval for printing loss (iterations)\n",
        "batch_size = 32\n",
        "n_iter = 30\n",
        "N_way = 50 # how many classes for testing one-shot tasks>\n",
        "n_val = 30 #how mahy one-shot tasks to validate on?\n",
        "best = -1\n",
        "data_path = \"./drive/My Drive/Colab Notebooks\"\n",
        "weights_path= CHECKPOINTED_WEIGHTS\n",
        "print(\"Starting training process!\")\n",
        "print(\"-------------------------------------\")\n",
        "t_start = time.time()\n",
        "for i in range(1, n_iter):\n",
        "    (inputs,targets)=get_batch(Xtrain,batch_size)\n",
        "    loss=siamese.train_on_batch(inputs,targets)\n",
        "    print(loss)\n",
        "    if i % evaluate_every == 0:\n",
        "        print(\"Time for {0} iterations: {1}\".format(i, time.time()-t_start))\n",
        "        val_acc = test_oneshot(siamese,N_way,n_val,Xval,verbose=True)\n",
        "        if val_acc >= best:\n",
        "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
        "            print(\"Saving weights to: {0} \\n\".format(weights_path))\n",
        "            siamese.save(weights_path)\n",
        "            best=val_acc\n",
        "\n",
        "    if i % loss_every == 0:\n",
        "        print(\"iteration {}, training loss: {:.2f},\".format(i,loss))\n",
        "# weights_path_2 = os.path.join(data_path, \"weight.h5\")\n",
        "# siamese.load_weights(CHECKPOINTED_WEIGHTS)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\n",
            "Starting training process!\n",
            "-------------------------------------\n",
            "[0.29838386, 0.5]\n",
            "Time for 1 iterations: 0.06723451614379883\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 70.0% 50 way one-shot learning accuracy\n",
            "Current best: 70.0, previous best: -1\n",
            "Saving weights to: ./drive/My Drive/checkpointed_weights_omniglot.hdf5 \n",
            "\n",
            "[0.29799283, 0.5]\n",
            "Time for 2 iterations: 1.751222848892212\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 70.0% 50 way one-shot learning accuracy\n",
            "Current best: 70.0, previous best: 70.0\n",
            "Saving weights to: ./drive/My Drive/checkpointed_weights_omniglot.hdf5 \n",
            "\n",
            "[0.298381, 0.5]\n",
            "Time for 3 iterations: 3.259033679962158\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 80.0% 50 way one-shot learning accuracy\n",
            "Current best: 80.0, previous best: 70.0\n",
            "Saving weights to: ./drive/My Drive/checkpointed_weights_omniglot.hdf5 \n",
            "\n",
            "[0.29902518, 0.5]\n",
            "Time for 4 iterations: 4.751343011856079\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 86.66666666666667% 50 way one-shot learning accuracy\n",
            "Current best: 86.66666666666667, previous best: 80.0\n",
            "Saving weights to: ./drive/My Drive/checkpointed_weights_omniglot.hdf5 \n",
            "\n",
            "[0.2984553, 0.5]\n",
            "Time for 5 iterations: 6.235727548599243\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 86.66666666666667% 50 way one-shot learning accuracy\n",
            "Current best: 86.66666666666667, previous best: 86.66666666666667\n",
            "Saving weights to: ./drive/My Drive/checkpointed_weights_omniglot.hdf5 \n",
            "\n",
            "[0.29868948, 0.5]\n",
            "Time for 6 iterations: 7.747466564178467\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 73.33333333333333% 50 way one-shot learning accuracy\n",
            "[0.29880714, 0.5]\n",
            "Time for 7 iterations: 9.20231556892395\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 80.0% 50 way one-shot learning accuracy\n",
            "[0.29588965, 0.5]\n",
            "Time for 8 iterations: 10.63491702079773\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 73.33333333333333% 50 way one-shot learning accuracy\n",
            "[0.29636225, 0.5]\n",
            "Time for 9 iterations: 12.161340713500977\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n",
            "Current best: 90.0, previous best: 86.66666666666667\n",
            "Saving weights to: ./drive/My Drive/checkpointed_weights_omniglot.hdf5 \n",
            "\n",
            "[0.29730305, 0.96875]\n",
            "Time for 10 iterations: 13.647337198257446\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 80.0% 50 way one-shot learning accuracy\n",
            "[0.29636595, 1.0]\n",
            "Time for 11 iterations: 15.179216861724854\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 80.0% 50 way one-shot learning accuracy\n",
            "[0.2966934, 1.0]\n",
            "Time for 12 iterations: 16.74041485786438\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 80.0% 50 way one-shot learning accuracy\n",
            "[0.29842198, 1.0]\n",
            "Time for 13 iterations: 18.228996753692627\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n",
            "Current best: 90.0, previous best: 90.0\n",
            "Saving weights to: ./drive/My Drive/checkpointed_weights_omniglot.hdf5 \n",
            "\n",
            "[0.29559603, 1.0]\n",
            "Time for 14 iterations: 19.76141119003296\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 86.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.29519063, 1.0]\n",
            "Time for 15 iterations: 21.226613759994507\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 86.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.29489163, 1.0]\n",
            "Time for 16 iterations: 22.808878660202026\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 80.0% 50 way one-shot learning accuracy\n",
            "[0.29616252, 1.0]\n",
            "Time for 17 iterations: 24.388261318206787\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 86.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.2970437, 1.0]\n",
            "Time for 18 iterations: 25.950829029083252\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 73.33333333333333% 50 way one-shot learning accuracy\n",
            "[0.29534328, 1.0]\n",
            "Time for 19 iterations: 27.515543937683105\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 86.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.29268703, 1.0]\n",
            "Time for 20 iterations: 29.07947540283203\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 93.33333333333333% 50 way one-shot learning accuracy\n",
            "Current best: 93.33333333333333, previous best: 90.0\n",
            "Saving weights to: ./drive/My Drive/checkpointed_weights_omniglot.hdf5 \n",
            "\n",
            "[0.29596034, 0.96875]\n",
            "Time for 21 iterations: 30.924147367477417\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 86.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.29419747, 1.0]\n",
            "Time for 22 iterations: 32.43948674201965\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n",
            "[0.29622117, 0.9375]\n",
            "Time for 23 iterations: 34.01906681060791\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n",
            "[0.29373106, 1.0]\n",
            "Time for 24 iterations: 35.58721423149109\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n",
            "[0.2950078, 1.0]\n",
            "Time for 25 iterations: 37.193063497543335\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 86.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.29331437, 1.0]\n",
            "Time for 26 iterations: 38.70622134208679\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 93.33333333333333% 50 way one-shot learning accuracy\n",
            "Current best: 93.33333333333333, previous best: 93.33333333333333\n",
            "Saving weights to: ./drive/My Drive/checkpointed_weights_omniglot.hdf5 \n",
            "\n",
            "[0.29479864, 0.96875]\n",
            "Time for 27 iterations: 40.49486517906189\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n",
            "[0.2948428, 1.0]\n",
            "Time for 28 iterations: 41.97251582145691\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n",
            "[0.29314217, 1.0]\n",
            "Time for 29 iterations: 43.41075372695923\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sp_VH5z0LZCx",
        "outputId": "9ffda650-a566-4324-840a-138b061c6e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "print(inputs[1].shape)\n",
        "# plot_oneshot_task(inputs)\n",
        "p=siamese.predict(inputs)\n",
        "print(p)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 1, 105, 105)\n",
            "[[0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.5003297 ]\n",
            " [0.4750408 ]\n",
            " [0.47657776]\n",
            " [0.4794998 ]\n",
            " [0.48850816]\n",
            " [0.49278167]\n",
            " [0.48520684]\n",
            " [0.46882784]\n",
            " [0.4732679 ]\n",
            " [0.4939587 ]\n",
            " [0.4564135 ]\n",
            " [0.4640463 ]\n",
            " [0.491099  ]\n",
            " [0.47420925]\n",
            " [0.48517922]\n",
            " [0.48425993]\n",
            " [0.49493802]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}